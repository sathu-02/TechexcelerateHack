{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 1: Install Required Libraries\n",
    "# !pip install transformers evaluate opencv-python huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Import Libraries and Set Up the Environment\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0)}\" if device.type == \"cuda\" else \"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Configure Model and Dataset\n",
    "# Model and dataset configuration\n",
    "model_ckpt = \"MCG-NJU/videomae-large\"\n",
    "dataset_root_path = Path(\"datasets/data\")  # Replace with your dataset path\n",
    "resize_to = 224  # Resize frames to 224x224\n",
    "num_frames = 16  # Number of frames per video\n",
    "batch_size = 4  # Training batch size\n",
    "num_epochs = 5  # Number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Define Video Preprocessing\n",
    "def load_video_frames(video_path, num_frames=16):\n",
    "    \"\"\"Extract frames from video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_count // num_frames)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create a Dataset Class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, processor, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        video_frames = load_video_frames(video_path, num_frames=self.num_frames)\n",
    "        inputs = self.processor(video_frames, return_tensors=\"pt\").pixel_values\n",
    "        return {\"pixel_values\": inputs.squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Prepare the Dataset\n",
    "# Load video file paths\n",
    "all_video_file_paths = list(dataset_root_path.glob(\"*/*.mp4\"))\n",
    "\n",
    "# Assign binary labels: 0 (normal), 1 (anomaly)\n",
    "binary_labels = [0 if \"normal\" in str(path.parent.name).lower() else 1 for path in all_video_file_paths]\n",
    "\n",
    "# Split the dataset\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    all_video_file_paths, binary_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize datasets\n",
    "processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "train_dataset = VideoDataset(train_paths, train_labels, processor)\n",
    "test_dataset = VideoDataset(test_paths, test_labels, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Define the Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=2, ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1900/1900 1:03:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.466095</td>\n",
       "      <td>0.805263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.376649</td>\n",
       "      <td>0.852632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.616391</td>\n",
       "      <td>0.871053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.876599</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.847368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1900, training_loss=0.3296295874369772, metrics={'train_runtime': 3789.584, 'train_samples_per_second': 2.005, 'train_steps_per_second': 0.501, 'total_flos': 3.3371799996255437e+19, 'train_loss': 0.3296295874369772, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 8: Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 01:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.3766492009162903, 'eval_accuracy': 0.8526315789473684, 'eval_runtime': 116.8818, 'eval_samples_per_second': 3.251, 'eval_steps_per_second': 0.813, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fine_tuned_anomaly_detector/preprocessor_config.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 10: Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./fine_tuned_anomaly_detector\")\n",
    "processor.save_pretrained(\"./fine_tuned_anomaly_detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b425686a6b646c3aa9c71c6a0876d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a261f1fbd14704a02685fbe0cfd88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rish13/anomaly-detector-videomae/commit/89757aeede6ddcbfcdbebfe36f34099922ae4469', commit_message='Upload processor', commit_description='', oid='89757aeede6ddcbfcdbebfe36f34099922ae4469', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rish13/anomaly-detector-videomae', endpoint='https://huggingface.co', repo_type='model', repo_id='rish13/anomaly-detector-videomae'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 11: Push to Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")  # Replace with your Hugging Face token\n",
    "\n",
    "model.push_to_hub(\"rish13/anomaly-detector-videomae\")\n",
    "processor.push_to_hub(\"rish13/anomaly-detector-videomae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(model, video_path, processor, num_frames=16):\n",
    "    video_frames = load_video_frames(video_path, num_frames)\n",
    "    inputs = processor(video_frames, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    timestamps = []\n",
    "    \n",
    "    # Iterate through predictions and mark anomalies (assume non-normal class is anomaly)\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        if prediction != 0:  # Assuming '0' is the 'normal' class\n",
    "            timestamp = idx * (1/30)  # Assuming 30 fps\n",
    "            timestamps.append((timestamp, video_frames[idx]))  # Store (timestamp, frame)\n",
    "    \n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect_anomaly_with_bounding_box(model, video_path, processor, num_frames=16):\n",
    "    timestamps = detect_anomaly(model, video_path, processor, num_frames)\n",
    "    \n",
    "    for timestamp, frame in timestamps:\n",
    "        print(f\"Anomaly detected at {timestamp:.2f} seconds\")\n",
    "\n",
    "        # Draw bounding box around the anomaly (example: full-frame bounding box)\n",
    "        frame_with_box = cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(f\"Anomaly at {timestamp:.2f}s\", frame_with_box)\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_video_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m processor \u001b[38;5;241m=\u001b[39m VideoMAEImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrish13/anomaly-detector-videomae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m VideoMAEForVideoClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrish13/anomaly-detector-videomae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdetect_anomaly_with_bounding_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mdetect_anomaly_with_bounding_box\u001b[0;34m(model, video_path, processor, num_frames)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_anomaly_with_bounding_box\u001b[39m(model, video_path, processor, num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_anomaly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m timestamp, frame \u001b[38;5;129;01min\u001b[39;00m timestamps:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly detected at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mdetect_anomaly\u001b[0;34m(model, video_path, processor, num_frames)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_anomaly\u001b[39m(model, video_path, processor, num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m \u001b[43mload_video_frames\u001b[49m(video_path, num_frames)\n\u001b[1;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(video_frames, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_video_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path to the video\n",
    "video_path = \"datasets/data/abuse/Abuse045_x264.mp4\"  # Path to your test video\n",
    "\n",
    "# Run anomaly detection with bounding box and timestamps\n",
    "processor = VideoMAEImageProcessor.from_pretrained(\"rish13/anomaly-detector-videomae\")\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\"rish13/anomaly-detector-videomae\")\n",
    "detect_anomaly_with_bounding_box(model, video_path, processor)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
