{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 1: Install Required Libraries\n",
    "# !pip install transformers evaluate opencv-python huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# print(torch.__version__, torchvision.__version__)\n",
    "#print(accelerate.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Import Libraries and Set Up the Environment\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0)}\" if device.type == \"cuda\" else \"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Configure Model and Dataset\n",
    "# Model and dataset configuration\n",
    "model_ckpt = \"MCG-NJU/videomae-large\"\n",
    "dataset_root_path = Path(\"train_70\")  # Replace with your dataset path\n",
    "resize_to = 224  # Resize frames to 224x224\n",
    "num_frames = 16  # Number of frames per video\n",
    "batch_size = 4  # Training batch size\n",
    "num_epochs = 10  # Number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Define Video Preprocessing\n",
    "def load_video_frames(video_path, num_frames=16):\n",
    "    \"\"\"Extract frames from video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for i in range(num_frames):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_count // num_frames)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create a Dataset Class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, processor, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        video_frames = load_video_frames(video_path, num_frames=self.num_frames)\n",
    "        inputs = self.processor(video_frames, return_tensors=\"pt\").pixel_values\n",
    "        return {\"pixel_values\": inputs.squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Prepare the Dataset\n",
    "# Load video file paths\n",
    "all_video_file_paths = list(dataset_root_path.glob(\"*/*.mp4\"))\n",
    "\n",
    "# Assign binary labels: 0 (normal), 1 (anomaly)\n",
    "binary_labels = [0 if \"normal\" in str(path.parent.name).lower() else 1 for path in all_video_file_paths]\n",
    "\n",
    "# Split the dataset\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    all_video_file_paths, binary_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize datasets\n",
    "processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "train_dataset = VideoDataset(train_paths, train_labels, processor)\n",
    "test_dataset = VideoDataset(test_paths, test_labels, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Define the Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=2, ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2590' max='2590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2590/2590 1:30:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.662732</td>\n",
       "      <td>0.552124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.701600</td>\n",
       "      <td>0.621963</td>\n",
       "      <td>0.733591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.626484</td>\n",
       "      <td>0.818533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.886502</td>\n",
       "      <td>0.841699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.146523</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.354682</td>\n",
       "      <td>0.814672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.182351</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.483970</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.506934</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.515279</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2590, training_loss=0.1755545456667204, metrics={'train_runtime': 5461.0965, 'train_samples_per_second': 1.897, 'train_steps_per_second': 0.474, 'total_flos': 4.549103262647452e+19, 'train_loss': 0.1755545456667204, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 8: Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 01:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.6219627261161804, 'eval_accuracy': 0.7335907335907336, 'eval_runtime': 88.4879, 'eval_samples_per_second': 2.927, 'eval_steps_per_second': 0.735, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./fine_tuned_anomaly_detector_10/preprocessor_config.json']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 10: Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./fine_tuned_anomaly_detector_10\")\n",
    "processor.save_pretrained(\"./fine_tuned_anomaly_detector_10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b997eba9e59456ab9c39c5250ab528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b5d63d7bc40cf8066250885e9c969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Sathwik-kom/anomaly-detector-videomae10/commit/a5d4e92c098b21252c13ad26c363d2a7803af790', commit_message='Upload processor', commit_description='', oid='a5d4e92c098b21252c13ad26c363d2a7803af790', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Sathwik-kom/anomaly-detector-videomae10', endpoint='https://huggingface.co', repo_type='model', repo_id='Sathwik-kom/anomaly-detector-videomae10'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 11: Push to Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")  # Replace with your Hugging Face token\n",
    "\n",
    "model.push_to_hub(\"Sathwik-kom/anomaly-detector-videomae10\")\n",
    "processor.push_to_hub(\"Sathwik-kom/anomaly-detector-videomae10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(model, video_path, processor, num_frames=16):\n",
    "    video_frames = load_video_frames(video_path, num_frames)\n",
    "    inputs = processor(video_frames, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    timestamps = []\n",
    "    \n",
    "    # Iterate through predictions and mark anomalies (assume non-normal class is anomaly)\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        if prediction != 0:  # Assuming '0' is the 'normal' class\n",
    "            timestamp = idx * (1/30)  # Assuming 30 fps\n",
    "            timestamps.append((timestamp, video_frames[idx]))  # Store (timestamp, frame)\n",
    "    \n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect_anomaly_with_bounding_box(model, video_path, processor, num_frames=16):\n",
    "    timestamps = detect_anomaly(model, video_path, processor, num_frames)\n",
    "    \n",
    "    for timestamp, frame in timestamps:\n",
    "        print(f\"Anomaly detected at {timestamp:.2f} seconds\")\n",
    "\n",
    "        # Draw bounding box around the anomaly (example: full-frame bounding box)\n",
    "        frame_with_box = cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(f\"Anomaly at {timestamp:.2f}s\", frame_with_box)\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9536139369010925, 'label': 'LABEL_1'},\n",
       " {'score': 0.046386029571294785, 'label': 'LABEL_0'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"video-classification\", model=\"Sathwik-kom/anomaly-detector-videomae10\")\n",
    "pipe(\"video8.mp4\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
